{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e6309f-a116-41cf-9e4c-ed2cf39adc1d",
   "metadata": {},
   "source": [
    "# Window Method in PyTorch\n",
    "\n",
    "Di bagian sebelumnya, kami membuat metode untuk menghitung sentimen rata-rata untuk potongan teks yang panjang dengan memecah teks menjadi beberapa jendela dan menghitung sentimen untuk setiap jendela satu per satu.\n",
    "\n",
    "Pendekatan kami di bagian terakhir adalah solusi cepat dan kotor. Di sini, kami akan berupaya meningkatkan proses ini dan mengimplementasikannya hanya menggunakan fungsi PyTorch untuk meningkatkan efisiensi.\n",
    "\n",
    "Hal pertama yang akan kita lakukan adalah mengimpor modul dan menginisialisasi model dan tokenizer kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bc4f19-bf97-44ea-8dc6-48eed651d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8583cbc3-3c03-4d8e-af8d-c4160a5e0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4857dc96-7f36-4116-a820-84215c0b0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "I would like to get your all  thoughts on the bond yield increase this week.  I am not worried about the market downturn but the sudden increase in yields. On 2/16 the 10 year bonds yields increased by almost  9 percent and on 2/19 the yield increased by almost 5 percent.\n",
    "\n",
    "Key Points from the CNBC Article:\n",
    "\n",
    "* **The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.**\n",
    "* **Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic.**\n",
    "* **However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.**\n",
    "\n",
    "The recent rise in bond yields and U.S. inflation expectations has some investors wary that a repeat of the 2013 “taper tantrum” could be on the horizon.\n",
    "\n",
    "The benchmark U.S. 10-year Treasury note climbed above 1.3% for the first time since February 2020 earlier this week, while the 30-year bond also hit its highest level for a year. Yields move inversely to bond prices.\n",
    "\n",
    "Yields tend to rise in lockstep with inflation expectations, which have reached their highest levels in a decade in the U.S., powered by increased prospects of a large fiscal stimulus package, progress on vaccine rollouts and pent-up consumer demand.\n",
    "\n",
    "The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.\n",
    "\n",
    "Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic. The Fed and others have maintained supportive tones in recent policy meetings, vowing to keep financial conditions loose as the global economy looks to emerge from the Covid-19 pandemic.\n",
    "\n",
    "However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.\n",
    "\n",
    "With central bank support removed, bonds usually fall in price which sends yields higher. This can also spill over into stock markets as higher interest rates means more debt servicing for firms, causing traders to reassess the investing environment.\n",
    "\n",
    "“The supportive stance from policymakers will likely remain in place until the vaccines have paved a way to some return to normality,” said Shane Balkham, chief investment officer at Beaufort Investment, in a research note this week.\n",
    "\n",
    "“However, there will be a risk of another ‘taper tantrum’ similar to the one we witnessed in 2013, and this is our main focus for 2021,” Balkham projected, should policymakers begin to unwind this stimulus.\n",
    "\n",
    "Long-term bond yields in Japan and Europe followed U.S. Treasurys higher toward the end of the week as bondholders shifted their portfolios.\n",
    "\n",
    "“The fear is that these assets are priced to perfection when the ECB and Fed might eventually taper,” said Sebastien Galy, senior macro strategist at Nordea Asset Management, in a research note entitled “Little taper tantrum.”\n",
    "\n",
    "“The odds of tapering are helped in the United States by better retail sales after four months of disappointment and the expectation of large issuance from the $1.9 trillion fiscal package.”\n",
    "\n",
    "Galy suggested the Fed would likely extend the duration on its asset purchases, moderating the upward momentum in inflation.\n",
    "\n",
    "“Equity markets have reacted negatively to higher yield as it offers an alternative to the dividend yield and a higher discount to long-term cash flows, making them focus more on medium-term growth such as cyclicals” he said. Cyclicals are stocks whose performance tends to align with economic cycles.\n",
    "\n",
    "Galy expects this process to be more marked in the second half of the year when economic growth picks up, increasing the potential for tapering.\n",
    "\n",
    "## Tapering in the U.S., but not Europe\n",
    "\n",
    "Allianz CEO Oliver Bäte told CNBC on Friday that there was a geographical divergence in how the German insurer is thinking about the prospect of interest rate hikes.\n",
    "\n",
    "“One is Europe, where we continue to have financial repression, where the ECB continues to buy up to the max in order to minimize spreads between the north and the south — the strong balance sheets and the weak ones — and at some point somebody will have to pay the price for that, but in the short term I don’t see any spike in interest rates,” Bäte said, adding that the situation is different stateside.\n",
    "\n",
    "“Because of the massive programs that have happened, the stimulus that is happening, the dollar being the world’s reserve currency, there is clearly a trend to stoke inflation and it is going to come. Again, I don’t know when and how, but the interest rates have been steepening and they should be steepening further.”\n",
    "\n",
    "## Rising yields a ‘normal feature’\n",
    "\n",
    "However, not all analysts are convinced that the rise in bond yields is material for markets. In a note Friday, Barclays Head of European Equity Strategy Emmanuel Cau suggested that rising bond yields were overdue, as they had been lagging the improving macroeconomic outlook for the second half of 2021, and said they were a “normal feature” of economic recovery.\n",
    "\n",
    "“With the key drivers of inflation pointing up, the prospect of even more fiscal stimulus in the U.S. and pent up demand propelled by high excess savings, it seems right for bond yields to catch-up with other more advanced reflation trades,” Cau said, adding that central banks remain “firmly on hold” given the balance of risks.\n",
    "\n",
    "He argued that the steepening yield curve is “typical at the early stages of the cycle,” and that so long as vaccine rollouts are successful, growth continues to tick upward and central banks remain cautious, reflationary moves across asset classes look “justified” and equities should be able to withstand higher rates.\n",
    "\n",
    "“Of course, after the strong move of the last few weeks, equities could mark a pause as many sectors that have rallied with yields look overbought, like commodities and banks,” Cau said.\n",
    "\n",
    "“But at this stage, we think rising yields are more a confirmation of the equity bull market than a threat, so dips should continue to be bought.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a3245-e317-4701-afc1-080e3aa182eb",
   "metadata": {},
   "source": [
    "This time, because we are using PyTorch, we will specify return_tensors='pt' when encoding our input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c502d830-9d5e-4a09-bfaa-eec7ad3791ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1045, 2052, 2066,  ..., 4149, 1012, 1524]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(txt, add_special_tokens=False,\n",
    "                               return_tensors='pt') # pytorch tensor\n",
    "\n",
    "print(len(tokens['input_ids'][0]))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb97ac0-91c6-41e8-9266-6415fb745a73",
   "metadata": {},
   "source": [
    "Now we have a set of tensors where each tensor contains 1345 tokens. We will use a similiar approach to what we used before where we will pull out a length of 510 tokens (or less), add the CLS and SEP tokens, then add PAD tokens when needed. To create these tensors of length 510 we need to use the torch.split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179a002d-c9aa-4d4a-bf89-fb33fc8b0915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c42cf07-32c4-4360-92f1-e0ab9ce43b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([4, 5, 6, 7]), tensor([8, 9]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c3f4d6-1eeb-40fc-81ae-b2b2e8eb3ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1045, 2052, 2066,  ..., 4149, 1012, 1524])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdee311-acd1-45b2-bb49-05df56f93942",
   "metadata": {},
   "source": [
    "Attention mask terdiri dari nilai biner (0 atau 1) dengan ukuran yang sama dengan input sequence. Nilai 1 menunjukkan posisi token yang valid, sementara nilai 0 menunjukkan posisi token yang di-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4910e76a-6e75-4453-afb6-14aeaf44bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_chunks = tokens['input_ids'][0].split(510) # split one text into 3 text\n",
    "mask_chunks = tokens['attention_mask'][0].split(510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4b2138-dcc1-4d5a-846f-e2de2241cc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 510, 325)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_id_chunks[0]), len(input_id_chunks[1]), len(input_id_chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29f007-9cbd-4afc-9e89-d8a727a4837a",
   "metadata": {},
   "source": [
    "To add our CLS (101) and SEP (102) tokens, we can use the torch.cat method. This method takes a list of tensors and concatenates them. Let's try it on our example tensor a first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d0d1ee-b6e2-45a7-b49a-77072cd694aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 102.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.cat(\n",
    "    [torch.Tensor([101]), a, torch.Tensor([102])]\n",
    ")\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f0357-1b5b-496e-8c0c-1494b046fa3e",
   "metadata": {},
   "source": [
    "It's that easy! We're almost there now, but we still need to add padding to our tensors to push them upto a length of 512, which should only be required for the final chunk. To do this we will build an if-statement that checks if the tensor length requires padding, and if so add the correct amount of padding which will be something like required_len = 512 - tensor_len. Again, let's test it on tensor a first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e71c923-d373-43ce-a329-614439d589df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_len = 20 - a.shape[0]\n",
    "\n",
    "padding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "326726ba-3ae3-43ee-add5-8c385e59ca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 102.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add padding so that the text is the same length\n",
    "\n",
    "if padding_len > 0:\n",
    "    a = torch.cat(\n",
    "        [a, torch.Tensor([0] * padding_len)]\n",
    "    )\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805df9fe-6c8c-4934-8343-6f1f7ffb9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_id_chunks) # tuple, so we must convert to list, so it's easy to loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d3098f8-0aa7-48dc-a696-84d33c30dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101.,  1045.,  2052.,  2066.,  2000.,  2131.,  2115.,  2035.,  4301.,\n",
      "         2006.,  1996.,  5416., 10750.,  3623.,  2023.,  2733.,  1012.,  1045.,\n",
      "         2572.,  2025.,  5191.,  2055.,  1996.,  3006.,  2091., 22299.,  2021.,\n",
      "         1996.,  5573.,  3623.,  1999., 16189.,  1012.,  2006.,  1016.,  1013.,\n",
      "         2385.,  1996.,  2184.,  2095.,  9547., 16189.,  3445.,  2011.,  2471.,\n",
      "         1023.,  3867.,  1998.,  2006.,  1016.,  1013.,  2539.,  1996., 10750.,\n",
      "         3445.,  2011.,  2471.,  1019.,  3867.,  1012.,  3145.,  2685.,  2013.,\n",
      "         1996., 27166.,  9818.,  3720.,  1024.,  1008.,  1008.,  1008.,  1996.,\n",
      "         1523.,  6823.,  2099.,  9092., 24456.,  1524.,  1999.,  2286.,  2001.,\n",
      "         1037.,  5573.,  9997.,  1999.,  9837., 16189.,  2349.,  2000.,  3006.,\n",
      "         6634.,  2044.,  1996.,  2976.,  3914.,  2623.,  2008.,  2009.,  2052.,\n",
      "         4088.,  6823.,  4892.,  2049., 20155., 24070.,  2565.,  1012.,  1008.,\n",
      "         1008.,  1008.,  1008.,  1008.,  2350.,  2430.,  5085.,  2105.,  1996.,\n",
      "         2088.,  2031.,  3013.,  3037.,  6165.,  2000.,  3181.,  2659.,  2015.,\n",
      "         1998.,  3390., 15741., 12450.,  1997., 11412., 17402.,  1999.,  1037.,\n",
      "         7226.,  2000.,  5370.,  2039.,  1996.,  4610.,  2802.,  1996.,  6090.,\n",
      "         3207.,  7712.,  1012.,  1008.,  1008.,  1008.,  1008.,  1008.,  2174.,\n",
      "         1010.,  1996.,  3522.,  4125.,  1999., 16189.,  6083.,  2008.,  2070.,\n",
      "         9387.,  2024.,  3225.,  2000.,  3424.,  6895., 17585.,  1037., 18711.,\n",
      "         1997.,  3343., 10076.,  2084., 11436.,  2000.,  8752.,  1037.,  4022.,\n",
      "         4125.,  1999., 14200.,  1012.,  1008.,  1008.,  1996.,  3522.,  4125.,\n",
      "         1999.,  5416., 16189.,  1998.,  1057.,  1012.,  1055.,  1012., 14200.,\n",
      "        10908.,  2038.,  2070.,  9387., 15705.,  2008.,  1037.,  9377.,  1997.,\n",
      "         1996.,  2286.,  1523.,  6823.,  2099.,  9092., 24456.,  1524.,  2071.,\n",
      "         2022.,  2006.,  1996.,  9154.,  1012.,  1996.,  6847., 10665.,  1057.,\n",
      "         1012.,  1055.,  1012.,  2184.,  1011.,  2095.,  9837.,  3602.,  6589.,\n",
      "         2682.,  1015.,  1012.,  1017.,  1003.,  2005.,  1996.,  2034.,  2051.,\n",
      "         2144.,  2337., 12609.,  3041.,  2023.,  2733.,  1010.,  2096.,  1996.,\n",
      "         2382.,  1011.,  2095.,  5416.,  2036.,  2718.,  2049.,  3284.,  2504.,\n",
      "         2005.,  1037.,  2095.,  1012., 16189.,  2693., 19262.,  2135.,  2000.,\n",
      "         5416.,  7597.,  1012., 16189.,  7166.,  2000.,  4125.,  1999., 11223.,\n",
      "         2618.,  2361.,  2007., 14200., 10908.,  1010.,  2029.,  2031.,  2584.,\n",
      "         2037.,  3284.,  3798.,  1999.,  1037.,  5476.,  1999.,  1996.,  1057.,\n",
      "         1012.,  1055.,  1012.,  1010.,  6113.,  2011.,  3445., 16746.,  1997.,\n",
      "         1037.,  2312., 10807., 19220.,  7427.,  1010.,  5082.,  2006., 17404.,\n",
      "         4897., 12166.,  1998.,  7279.,  2102.,  1011.,  2039.,  7325.,  5157.,\n",
      "         1012.,  1996.,  1523.,  6823.,  2099.,  9092., 24456.,  1524.,  1999.,\n",
      "         2286.,  2001.,  1037.,  5573.,  9997.,  1999.,  9837., 16189.,  2349.,\n",
      "         2000.,  3006.,  6634.,  2044.,  1996.,  2976.,  3914.,  2623.,  2008.,\n",
      "         2009.,  2052.,  4088.,  6823.,  4892.,  2049., 20155., 24070.,  2565.,\n",
      "         1012.,  2350.,  2430.,  5085.,  2105.,  1996.,  2088.,  2031.,  3013.,\n",
      "         3037.,  6165.,  2000.,  3181.,  2659.,  2015.,  1998.,  3390., 15741.,\n",
      "        12450.,  1997., 11412., 17402.,  1999.,  1037.,  7226.,  2000.,  5370.,\n",
      "         2039.,  1996.,  4610.,  2802.,  1996.,  6090.,  3207.,  7712.,  1012.,\n",
      "         1996.,  7349.,  1998.,  2500.,  2031.,  5224., 16408., 12623.,  1999.,\n",
      "         3522.,  3343.,  6295.,  1010., 19076.,  2075.,  2000.,  2562.,  3361.,\n",
      "         3785.,  6065.,  2004.,  1996.,  3795.,  4610.,  3504.,  2000., 12636.,\n",
      "         2013.,  1996.,  2522., 17258.,  1011.,  2539.,  6090.,  3207.,  7712.,\n",
      "         1012.,  2174.,  1010.,  1996.,  3522.,  4125.,  1999., 16189.,  6083.,\n",
      "         2008.,  2070.,  9387.,  2024.,  3225.,  2000.,  3424.,  6895., 17585.,\n",
      "         1037., 18711.,  1997.,  3343., 10076.,  2084., 11436.,  2000.,  8752.,\n",
      "         1037.,  4022.,  4125.,  1999., 14200.,  1012.,  2007.,  2430.,  2924.,\n",
      "         2490.,  3718.,  1010.,  9547.,  2788.,  2991.,  1999.,  3976.,  2029.,\n",
      "        10255., 16189.,  3020.,  1012.,  2023.,  2064.,  2036., 14437.,  2058.,\n",
      "         2046.,  4518.,  6089.,  2004.,  3020.,  3037.,  6165.,  2965.,  2062.,\n",
      "         7016., 26804.,  2005.,  9786.,  1010.,  4786., 13066.,  2000.,  2128.,\n",
      "        27241.,  4757.,  1996., 19920.,  4044.,  1012.,  1523.,   102.])\n",
      "512\n",
      "tensor([  101.,  1996., 16408., 11032.,  2013.,  3343., 12088.,  2097.,  3497.,\n",
      "         3961.,  1999.,  2173.,  2127.,  1996., 28896.,  2031., 12308.,  1037.,\n",
      "         2126.,  2000.,  2070.,  2709.,  2000.,  3671.,  3012.,  1010.,  1524.,\n",
      "         2056.,  8683., 28352., 15256.,  2213.,  1010.,  2708.,  5211.,  2961.,\n",
      "         2012., 23622.,  5211.,  1010.,  1999.,  1037.,  2470.,  3602.,  2023.,\n",
      "         2733.,  1012.,  1523.,  2174.,  1010.,  2045.,  2097.,  2022.,  1037.,\n",
      "         3891.,  1997.,  2178.,  1520.,  6823.,  2099.,  9092., 24456.,  1521.,\n",
      "         2714.,  2000.,  1996.,  2028.,  2057.,  9741.,  1999.,  2286.,  1010.,\n",
      "         1998.,  2023.,  2003.,  2256.,  2364.,  3579.,  2005., 25682.,  1010.,\n",
      "         1524., 28352., 15256.,  2213., 11310.,  1010.,  2323.,  3343., 12088.,\n",
      "         4088.,  2000.,  4895., 11101.,  2023., 19220.,  1012.,  2146.,  1011.,\n",
      "         2744.,  5416., 16189.,  1999.,  2900.,  1998.,  2885.,  2628.,  1057.,\n",
      "         1012.,  1055.,  1012.,  9837.,  2015.,  3020.,  2646.,  1996.,  2203.,\n",
      "         1997.,  1996.,  2733.,  2004.,  5416., 17794.,  5429.,  2037., 11103.,\n",
      "         2015.,  1012.,  1523.,  1996.,  3571.,  2003.,  2008.,  2122.,  7045.,\n",
      "         2024., 21125.,  2000., 15401.,  2043.,  1996., 14925.,  2497.,  1998.,\n",
      "         7349.,  2453.,  2776.,  6823.,  2099.,  1010.,  1524.,  2056., 28328.,\n",
      "        14891.,  2100.,  1010.,  3026., 26632.,  2358., 11657., 24063.,  2012.,\n",
      "        13926.,  5243., 11412.,  2968.,  1010.,  1999.,  1037.,  2470.,  3602.,\n",
      "         4709.,  1523.,  2210.,  6823.,  2099.,  9092., 24456.,  1012.,  1524.,\n",
      "         1523.,  1996., 10238.,  1997.,  6823.,  4892.,  2024.,  3271.,  1999.,\n",
      "         1996.,  2142.,  2163.,  2011.,  2488.,  7027.,  4341.,  2044.,  2176.,\n",
      "         2706.,  1997., 10520.,  1998.,  1996., 17626.,  1997.,  2312., 26354.,\n",
      "        26620.,  2013.,  1996.,  1002.,  1015.,  1012.,  1023., 23458., 10807.,\n",
      "         7427.,  1012.,  1524., 14891.,  2100.,  4081.,  1996.,  7349.,  2052.,\n",
      "         3497.,  7949.,  1996.,  9367.,  2006.,  2049., 11412., 17402.,  1010.,\n",
      "         5549., 15172.,  1996., 10745., 11071.,  1999., 14200.,  1012.,  1523.,\n",
      "        10067.,  6089.,  2031., 14831., 19762.,  2000.,  3020., 10750.,  2004.,\n",
      "         2009.,  4107.,  2019.,  4522.,  2000.,  1996., 11443.,  4859., 10750.,\n",
      "         1998.,  1037.,  3020., 19575.,  2000.,  2146.,  1011.,  2744.,  5356.,\n",
      "         6223.,  1010.,  2437.,  2068.,  3579.,  2062.,  2006.,  5396.,  1011.,\n",
      "         2744.,  3930.,  2107.,  2004., 23750.,  9777.,  1524.,  2002.,  2056.,\n",
      "         1012., 23750.,  9777.,  2024., 15768.,  3005.,  2836., 12102.,  2000.,\n",
      "        25705.,  2007.,  3171., 12709.,  1012., 14891.,  2100., 24273.,  2023.,\n",
      "         2832.,  2000.,  2022.,  2062.,  4417.,  1999.,  1996.,  2117.,  2431.,\n",
      "         1997.,  1996.,  2095.,  2043.,  3171.,  3930., 11214.,  2039.,  1010.,\n",
      "         4852.,  1996.,  4022.,  2005.,  6823.,  4892.,  1012.,  1001.,  1001.,\n",
      "         6823.,  4892.,  1999.,  1996.,  1057.,  1012.,  1055.,  1012.,  1010.,\n",
      "         2021.,  2025.,  2885.,  2035.,  2937.,  2480.,  5766.,  6291.,  7151.,\n",
      "         2063.,  2409., 27166.,  9818.,  2006.,  5958.,  2008.,  2045.,  2001.,\n",
      "         1037., 10056., 17856., 17905.,  1999.,  2129.,  1996.,  2446., 16021.,\n",
      "        27595.,  2003.,  3241.,  2055.,  1996.,  9824.,  1997.,  3037.,  3446.,\n",
      "        21857.,  2015.,  1012.,  1523.,  2028.,  2003.,  2885.,  1010.,  2073.,\n",
      "         2057.,  3613.,  2000.,  2031.,  3361., 22422.,  1010.,  2073.,  1996.,\n",
      "        14925.,  2497.,  4247.,  2000.,  4965.,  2039.,  2000.,  1996.,  4098.,\n",
      "         1999.,  2344.,  2000., 18478., 20861.,  2090.,  1996.,  2167.,  1998.,\n",
      "         1996.,  2148.,  1517.,  1996.,  2844.,  5703.,  8697.,  1998.,  1996.,\n",
      "         5410.,  3924.,  1517.,  1998.,  2012.,  2070.,  2391.,  8307.,  2097.,\n",
      "         2031.,  2000.,  3477.,  1996.,  3976.,  2005.,  2008.,  1010.,  2021.,\n",
      "         1999.,  1996.,  2460.,  2744.,  1045.,  2123.,  1521.,  1056.,  2156.,\n",
      "         2151.,  9997.,  1999.,  3037.,  6165.,  1010.,  1524.,  7151.,  2063.,\n",
      "         2056.,  1010.,  5815.,  2008.,  1996.,  3663.,  2003.,  2367.,  2163.,\n",
      "         5178.,  1012.,  1523.,  2138.,  1997.,  1996.,  5294.,  3454.,  2008.,\n",
      "         2031.,  3047.,  1010.,  1996., 19220.,  2008.,  2003.,  6230.,  1010.,\n",
      "         1996.,  7922.,  2108.,  1996.,  2088.,  1521.,  1055.,  3914.,  9598.,\n",
      "         1010.,  2045.,  2003.,  4415.,  1037.,  9874.,  2000., 13299., 14200.,\n",
      "         1998.,  2009.,  2003.,  2183.,  2000.,  2272.,  1012.,   102.])\n",
      "512\n",
      "tensor([  101.,  2153.,  1010.,  1045.,  2123.,  1521.,  1056.,  2113.,  2043.,\n",
      "         1998.,  2129.,  1010.,  2021.,  1996.,  3037.,  6165.,  2031.,  2042.,\n",
      "         9561.,  7406.,  1998.,  2027.,  2323.,  2022.,  9561.,  7406.,  2582.,\n",
      "         1012.,  1524.,  1001.,  1001.,  4803., 16189.,  1037.,  1520.,  3671.,\n",
      "         3444.,  1521.,  2174.,  1010.,  2025.,  2035., 18288.,  2024.,  6427.,\n",
      "         2008.,  1996.,  4125.,  1999.,  5416., 16189.,  2003.,  3430.,  2005.,\n",
      "         6089.,  1012.,  1999.,  1037.,  3602.,  5958.,  1010., 23724.,  2015.,\n",
      "         2132.,  1997.,  2647., 10067.,  5656., 14459.,  6187.,  2226.,  4081.,\n",
      "         2008.,  4803.,  5416., 16189.,  2020.,  2058., 20041.,  1010.,  2004.,\n",
      "         2027.,  2018.,  2042.,  2474., 12588.,  1996.,  9229., 26632., 23035.,\n",
      "        17680.,  2005.,  1996.,  2117.,  2431.,  1997., 25682.,  1010.,  1998.,\n",
      "         2056.,  2027.,  2020.,  1037.,  1523.,  3671.,  3444.,  1524.,  1997.,\n",
      "         3171.,  7233.,  1012.,  1523.,  2007.,  1996.,  3145.,  6853.,  1997.,\n",
      "        14200.,  7302.,  2039.,  1010.,  1996.,  9824.,  1997.,  2130.,  2062.,\n",
      "        10807., 19220.,  1999.,  1996.,  1057.,  1012.,  1055.,  1012.,  1998.,\n",
      "         7279.,  2102.,  2039.,  5157., 15801.,  2011.,  2152.,  9987., 10995.,\n",
      "         1010.,  2009.,  3849.,  2157.,  2005.,  5416., 16189.,  2000.,  4608.,\n",
      "         1011.,  2039.,  2007.,  2060.,  2062.,  3935., 25416., 13490., 14279.,\n",
      "         1010.,  1524.,  6187.,  2226.,  2056.,  1010.,  5815.,  2008.,  2430.,\n",
      "         5085.,  3961.,  1523.,  7933.,  2006.,  2907.,  1524.,  2445.,  1996.,\n",
      "         5703.,  1997., 10831.,  1012.,  2002.,  5275.,  2008.,  1996.,  9561.,\n",
      "         7406., 10750.,  7774.,  2003.,  1523.,  5171.,  2012.,  1996.,  2220.,\n",
      "         5711.,  1997.,  1996.,  5402.,  1010.,  1524.,  1998.,  2008.,  2061.,\n",
      "         2146.,  2004., 17404.,  4897., 12166.,  2024.,  3144.,  1010.,  3930.,\n",
      "         4247.,  2000., 16356., 10745.,  1998.,  2430.,  5085.,  3961., 17145.,\n",
      "         1010., 25416., 13490.,  5649.,  5829.,  2408., 11412.,  4280.,  2298.,\n",
      "         1523., 15123.,  1524.,  1998.,  1041., 15549.,  7368.,  2323.,  2022.,\n",
      "         2583.,  2000., 19319.,  3020.,  6165.,  1012.,  1523.,  1997.,  2607.,\n",
      "         1010.,  2044.,  1996.,  2844.,  2693.,  1997.,  1996.,  2197.,  2261.,\n",
      "         3134.,  1010.,  1041., 15549.,  7368.,  2071.,  2928.,  1037.,  8724.,\n",
      "         2004.,  2116., 11105.,  2008.,  2031., 24356.,  2007., 16189.,  2298.,\n",
      "         2058.,  5092., 18533.,  1010.,  2066., 21955.,  1998.,  5085.,  1010.,\n",
      "         1524.,  6187.,  2226.,  2056.,  1012.,  1523.,  2021.,  2012.,  2023.,\n",
      "         2754.,  1010.,  2057.,  2228.,  4803., 16189.,  2024.,  2062.,  1037.,\n",
      "        13964.,  1997.,  1996., 10067.,  7087.,  3006.,  2084.,  1037.,  5081.,\n",
      "         1010.,  2061., 16510.,  2015.,  2323.,  3613.,  2000.,  2022.,  4149.,\n",
      "         1012.,  1524.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# define target chunksize\n",
    "chunksize = 512\n",
    "\n",
    "# split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "# loop through each chunk\n",
    "for i in range(len(input_id_chunks)): # 510 len\n",
    "\n",
    "    # add CLS and SEP tokens to input IDs\n",
    "    \n",
    "    input_id_chunks[i] = torch.cat(\n",
    "        [ torch.Tensor([101]), input_id_chunks[i], torch.Tensor([102]) ]\n",
    "    )\n",
    "\n",
    "    # add attention tokens to attention mask\n",
    "    \n",
    "    mask_chunks[i] = torch.cat(\n",
    "        [ torch.Tensor([1]), mask_chunks[i], torch.Tensor([1]) ]\n",
    "    )\n",
    "\n",
    "    # get required padding length\n",
    "    \n",
    "    pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "\n",
    "    # check if tensor length satisfies required chunk size\n",
    "    \n",
    "    if pad_len > 0 :\n",
    "        input_id_chunks[i] = torch.cat(\n",
    "            [input_id_chunks[i], torch.Tensor([0] * pad_len) ]\n",
    "        )\n",
    "        \n",
    "        mask_chunks[i] = torch.cat(\n",
    "            [mask_chunks[i], torch.Tensor([0] * pad_len) ]\n",
    "        )\n",
    "        \n",
    "\n",
    "# check length of each tensor\n",
    "for chunk in input_id_chunks:\n",
    "    print(chunk), print(len(chunk))\n",
    "    \n",
    "# print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "# chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14c03e-b83b-4aa9-95c1-17600ac5e745",
   "metadata": {},
   "source": [
    "Now the final step of placing our tensors back into the dictionary style format we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "528a2169-5ad4-4538-a14a-e4d871703b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2052,  ...,  1012,  1523,   102],\n",
       "         [  101,  1996, 16408,  ...,  2272,  1012,   102],\n",
       "         [  101,  2153,  1010,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge tensor to new tensor with higher dimension\n",
    "\n",
    "input_ids = torch.stack(input_id_chunks)\n",
    "attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "input_dict = {\n",
    "    'input_ids': input_ids.long(),\n",
    "    'attention_mask': attention_mask.int()\n",
    "}\n",
    "\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893e073-aaee-46e3-9185-38d933e980ab",
   "metadata": {},
   "source": [
    "We can now process all chunks and calculate probabilities using softmax in parallel like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e498c2-c5e6-4589-9f30-0d768cf81e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2807,  1.4917, -1.3592],\n",
       "        [ 0.0747,  0.2922, -0.7954],\n",
       "        [ 1.0204, -0.2699, -1.3167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**input_dict)\n",
    "outputs # outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2eab4bcf-576b-46aa-97d8-8cab912aa966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1384, 0.8145, 0.0471],\n",
       "        [0.3757, 0.4670, 0.1574],\n",
       "        [0.7290, 0.2006, 0.0704]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "831c7ca0-7725-49db-8352-99984fa58fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4144, 0.4940, 0.0916], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = probs.mean(dim=0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c142d7-7f3d-4733-ab74-4b77b766a24a",
   "metadata": {},
   "source": [
    "dim=0 menentukan dimensi pada mana operasi mean akan dilakukan, dalam hal ini adalah dimensi batch.\n",
    "\n",
    "Dengan demikian, penggunaan probs.mean(dim=0) pada probabilitas yang sudah dihitung akan menghasilkan sebuah tensor dengan ukuran yang sama dengan setiap vektor representasi yang sudah dihitung. Tensor ini akan merepresentasikan rata-rata probabilitas dari setiap sequence atau dokumen dalam batch.\n",
    "\n",
    "maka PyTorch akan melakukan operasi mean pada dimensi pertama dari tensor tersebut, yaitu pada dimensi yang merepresentasikan batch. Oleh karena itu, hasil yang dihasilkan adalah tensor baru yang merepresentasikan rata-rata probabilitas dari setiap item dalam batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a2fb9f2-90eb-453f-b85c-9a7ee39385bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361efdc3-970c-491b-b4db-9d5d1057957c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
